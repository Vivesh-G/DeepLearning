{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F7B9dm1kLtnh"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cTLV_vDp4p3N"
      },
      "outputs": [],
      "source": [
        "#Activation Functions\n",
        "class sigmoid:\n",
        "  def forward(self, Z):\n",
        "    return 1 / (1 + np.exp(-Z)), Z\n",
        "\n",
        "  def backward(self, dA, Z):\n",
        "    forward_output, _ = self.forward(Z)\n",
        "    return dA * forward_output * (1 - forward_output)\n",
        "\n",
        "class Relu:\n",
        "  def forward(self, Z):\n",
        "    return np.maximum(0, Z), Z\n",
        "\n",
        "  def backward(self, dA, Z):\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jtu1zIBe_LYr"
      },
      "outputs": [],
      "source": [
        "#Dense Layers\n",
        "class Layer:\n",
        "  def __init__(self, size, activation, input_size):\n",
        "    self.size = size\n",
        "    self.activation = activation\n",
        "    self.W = np.random.randn(self.size, input_size) * 0.01 #Weights\n",
        "    self.b = np.zeros((self.size, 1)) #Biases\n",
        "\n",
        "  def forward(self, A_prev):\n",
        "    Z = np.dot(self.W, A_prev) + self.b\n",
        "    A, activation_cache = self.activation.forward(Z)\n",
        "    cache = (A_prev, self.W, self.b, activation_cache)\n",
        "    return A, cache\n",
        "\n",
        "  def backward(self, dA, cache):\n",
        "    A_prev, W, b, activation_cache = cache\n",
        "    dZ = self.activation.backward(dA, activation_cache)\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1 / m) * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pNwq1YF_EtLq"
      },
      "outputs": [],
      "source": [
        "#Neural Network\n",
        "\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, layer_dims, learning_rate):\n",
        "    self.Layers = []\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    for i in range(1, len(layer_dims)):\n",
        "      activation = Relu() if i < len(layer_dims) - 1 else sigmoid()\n",
        "      self.Layers.append(Layer(layer_dims[i], activation, layer_dims[i - 1]))\n",
        "\n",
        "  def forward_propagation(self, X):\n",
        "    A = X\n",
        "    caches = []\n",
        "    for layer in self.Layers:\n",
        "      A, cache = layer.forward(A)\n",
        "      caches.append(cache)\n",
        "    return A, caches\n",
        "\n",
        "  def compute_cost(slef, AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    e = 1e-15\n",
        "    AL = np.clip(AL, e, 1 - e)\n",
        "    cost = (-1 / m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
        "    return float(np.squeeze(cost))\n",
        "\n",
        "  def backward_propagation(self, AL, Y, caches):\n",
        "    Y = Y.reshape(AL.shape)\n",
        "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
        "\n",
        "    gradients = []\n",
        "    dA = dAL\n",
        "\n",
        "    for layer, cache in reversed(list(zip(self.Layers, caches))):\n",
        "      dA, dW, db = layer.backward(dA, cache)\n",
        "      gradients.append((dW, db))\n",
        "\n",
        "    return list(reversed(gradients))\n",
        "\n",
        "  def update_parameters(self, gradients):\n",
        "    for i, layer in enumerate(self.Layers):\n",
        "      dW, db = gradients[i]\n",
        "      layer.W -= self.learning_rate * dW\n",
        "      layer.b -= self.learning_rate * db\n",
        "\n",
        "  def train(self, X, Y, num_iterations):\n",
        "    costs = []\n",
        "    for i in range(num_iterations):\n",
        "      AL, caches = self.forward_propagation(X)\n",
        "      cost = self.compute_cost(AL, Y)\n",
        "      gradients = self.backward_propagation(AL, Y, caches)\n",
        "      self.update_parameters(gradients)\n",
        "      costs.append(cost)\n",
        "      if i % 100 == 0:\n",
        "        print(f\"Cost after iteration {i}: {cost}\")\n",
        "    return costs\n",
        "\n",
        "  def predict(self, X, threshold = 0.5):\n",
        "    AL, _ = self.forward_propagation(X)\n",
        "    predictions = (AL > threshold).astype(int)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccy2lIwOIePP",
        "outputId": "1bb23c46-5f67-4492-9073-85d714b0faba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.6931472236781854\n",
            "Cost after iteration 100: 0.691582435525438\n",
            "Cost after iteration 200: 0.6905072294439459\n",
            "Cost after iteration 300: 0.6897682428606673\n",
            "Cost after iteration 400: 0.6892601746351852\n",
            "Cost after iteration 500: 0.6889107478122627\n",
            "Cost after iteration 600: 0.6886703447216688\n",
            "Cost after iteration 700: 0.688504895816492\n",
            "Cost after iteration 800: 0.6883909978575475\n",
            "Cost after iteration 900: 0.6883125681923888\n",
            "Cost after iteration 1000: 0.6882585494485142\n",
            "Cost after iteration 1100: 0.6882213364083377\n",
            "Cost after iteration 1200: 0.6881956963056259\n",
            "Cost after iteration 1300: 0.6881780274832565\n",
            "Cost after iteration 1400: 0.6881658502346053\n",
            "Cost after iteration 1500: 0.6881574568064726\n",
            "Cost after iteration 1600: 0.688151670998625\n",
            "Cost after iteration 1700: 0.688147682393836\n",
            "Cost after iteration 1800: 0.6881449325777105\n",
            "Cost after iteration 1900: 0.6881430366943535\n",
            "Cost after iteration 2000: 0.6881417295024356\n",
            "Cost after iteration 2100: 0.6881408281716107\n",
            "Cost after iteration 2200: 0.6881402066671533\n",
            "Cost after iteration 2300: 0.6881397781005061\n",
            "Cost after iteration 2400: 0.688139482567923\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(1)\n",
        "X = np.random.randn(5, 100)  # 100 samples with 5 features\n",
        "Y = (np.random.randn(1, 100) > 0).astype(int)\n",
        "\n",
        "\n",
        "layer_dims = [5, 4, 3, 1]  # 4-layer model\n",
        "nn = NeuralNetwork(layer_dims, learning_rate=0.0075)\n",
        "\n",
        "\n",
        "costs = nn.train(X, Y, num_iterations=2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1yCI548dACY",
        "outputId": "92fb686c-7378-4ebb-e532-7aedef1364ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the neural network on a subset of MNIST...\n",
            "Cost after iteration 0: 6.931828854498803\n",
            "Cost after iteration 100: 5.662947294453207\n",
            "Cost after iteration 200: 4.853196181381265\n",
            "Cost after iteration 300: 4.21748157223142\n",
            "Cost after iteration 400: 3.5246640008211\n",
            "Cost after iteration 500: 3.2619701694394685\n",
            "Cost after iteration 600: 3.2335876351457427\n",
            "Cost after iteration 700: 3.212205463962775\n",
            "Cost after iteration 800: 3.1678420411922605\n",
            "Cost after iteration 900: 3.060260760963288\n",
            "Cost after iteration 1000: 2.873945367753305\n",
            "Cost after iteration 1100: 2.7133638699591383\n",
            "Cost after iteration 1200: 2.590873613542833\n",
            "Cost after iteration 1300: 2.4833465265540506\n",
            "Cost after iteration 1400: 2.3686205170482544\n",
            "Cost after iteration 1500: 2.216162780480342\n",
            "Cost after iteration 1600: 2.0222770768973577\n",
            "Cost after iteration 1700: 1.8381703062672086\n",
            "Cost after iteration 1800: 1.6540883838749825\n",
            "Cost after iteration 1900: 1.4357751586067633\n",
            "Cost after iteration 2000: 1.2172484500218284\n",
            "Cost after iteration 2100: 1.0359391767118193\n",
            "Cost after iteration 2200: 0.8818978129165598\n",
            "Cost after iteration 2300: 0.7523034495279807\n",
            "Cost after iteration 2400: 0.6480894951146264\n",
            "Cost after iteration 2500: 0.5664984729590588\n",
            "Cost after iteration 2600: 0.5012583717712292\n",
            "Cost after iteration 2700: 0.44706080143297877\n",
            "Cost after iteration 2800: 0.40064134212034086\n",
            "Cost after iteration 2900: 0.3597846733682438\n",
            "Cost after iteration 3000: 0.3224655319813132\n",
            "Cost after iteration 3100: 0.28725538010990853\n",
            "Cost after iteration 3200: 0.2539133064601862\n",
            "Cost after iteration 3300: 0.22266994143899735\n",
            "Cost after iteration 3400: 0.1940618706188622\n",
            "Cost after iteration 3500: 0.16826949981782627\n",
            "Cost after iteration 3600: 0.14537077927256303\n",
            "Cost after iteration 3700: 0.1256643339515197\n",
            "Cost after iteration 3800: 0.10898666507123883\n",
            "Cost after iteration 3900: 0.09504207897620184\n",
            "Cost after iteration 4000: 0.0833854388798244\n",
            "Cost after iteration 4100: 0.07362965562810361\n",
            "Cost after iteration 4200: 0.06539564599448774\n",
            "Cost after iteration 4300: 0.058447673484378154\n",
            "Cost after iteration 4400: 0.05256470334710224\n",
            "Cost after iteration 4500: 0.04757410675864421\n",
            "Cost after iteration 4600: 0.04331287987518017\n",
            "Cost after iteration 4700: 0.039637841710378145\n",
            "Cost after iteration 4800: 0.03644477805995517\n",
            "Cost after iteration 4900: 0.03365015404201126\n",
            "Test Accuracy: 74.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load MNIST data\n",
        "def load_mnist():\n",
        "    mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
        "    X = mnist.data.astype(np.float32)\n",
        "    Y = mnist.target.astype(np.int32)\n",
        "\n",
        "    # Normalize pixel values\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    Y = encoder.fit_transform(Y.reshape(-1, 1))\n",
        "\n",
        "    return X.T, Y.T\n",
        "\n",
        "def preprocess_data():\n",
        "    X, Y = load_mnist()\n",
        "\n",
        "    # Select only 500 samples for faster training and testing\n",
        "    X, _, Y, _ = train_test_split(X.T, Y.T, train_size=500, random_state=42)\n",
        "\n",
        "    # Split into training and testing subsets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "    return X_train.T, X_test.T, Y_train.T, Y_test.T\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Preprocess data\n",
        "    X_train, X_test, Y_train, Y_test = preprocess_data()\n",
        "\n",
        "    # Define the network architecture\n",
        "    layer_dims = [784, 64, 32, 10]  # 4 layer network for testing\n",
        "    nn = NeuralNetwork(layer_dims, learning_rate=0.01)\n",
        "\n",
        "    # Train the neural network\n",
        "    print(\"Training the neural network on a subset of MNIST...\")\n",
        "    costs = nn.train(X_train, Y_train, num_iterations=5000)\n",
        "\n",
        "    # Test the model\n",
        "    predictions = nn.predict(X_test)\n",
        "    accuracy = np.mean(np.argmax(predictions, axis=0) == np.argmax(Y_test, axis=0))\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
